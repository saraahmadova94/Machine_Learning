{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 wisker değerine göre her bir gruba ait outlier olabileceğini değerlendirdiğimiz gözlemleri tespit ediyoruz.\n",
    "\n",
    "total_outliers = []\n",
    "\n",
    "for model in df.make_model.unique():\n",
    "    \n",
    "    car_prices = df[df[\"make_model\"]== model][\"price\"]\n",
    "    \n",
    "    Q1 = car_prices.quantile(0.25)\n",
    "    Q3 = car_prices.quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    lower_lim = Q1-1.5*IQR\n",
    "    upper_lim = Q3+1.5*IQR\n",
    "    \n",
    "    count_of_outliers = (car_prices[(car_prices < lower_lim) | (car_prices > upper_lim)]).count()\n",
    "    \n",
    "    total_outliers.append(count_of_outliers)\n",
    "    \n",
    "    print(f\" The count of outlier for {model:<15} : {count_of_outliers:<5}, \\\n",
    "          The rate of outliers : {(count_of_outliers/len(df[df['make_model']== model])).round(3)}\")\n",
    "print()    \n",
    "print(\"Total_outliers : \",sum(total_outliers), \"The rate of total outliers :\", (sum(total_outliers)/len(df)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "visualizer = ClassPredictionError(pipe_model)\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "# Draw visualization\n",
    "visualizer.poof();\n",
    "#classpredictionerror fonksiyonu class bazında yapılan hataları gösteriyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setinin genel bir profilini elde etmek için\n",
    "# Gözlem sayısı-veri tipi-null değer sayısı-unique değer sayısı-min-max değerleri\n",
    "# null değerler \"0\" gözüküyor!\n",
    "\n",
    "def summary(df, pred=None):\n",
    "    obs = df.shape[0]\n",
    "    Types = df.dtypes\n",
    "    Counts = df.apply(lambda x: x.count())\n",
    "    Min = df.min()\n",
    "    Max = df.max()\n",
    "    Uniques = df.apply(lambda x: x.unique().shape[0])\n",
    "    Nulls = df.apply(lambda x: x.isnull().sum())\n",
    "    print('Data shape:', df.shape)\n",
    "\n",
    "    if pred is None:\n",
    "        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n",
    "        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n",
    "\n",
    "    str.columns = cols\n",
    "    print('___________________________\\nData Types:')\n",
    "    print(str.Types.value_counts())\n",
    "    print('___________________________')\n",
    "    return str\n",
    "\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setindeki yinelenen gözlemleri kontrol eder ve bunları kaldırır\n",
    "\n",
    "def duplicate_values(df):\n",
    "    print(\"Duplicate check...\")\n",
    "    num_duplicates = df.duplicated(subset=None, keep='first').sum()\n",
    "    if num_duplicates > 0:\n",
    "        print(\"There are\", num_duplicates, \"duplicated observations in the dataset.\")\n",
    "        df.drop_duplicates(keep='first', inplace=True)\n",
    "        print(num_duplicates, \"duplicates were dropped!\")\n",
    "        print(\"No more duplicate rows!\")\n",
    "    else:\n",
    "        print(\"There are no duplicated observations in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pipe_model[\"OrdinalEncoder\"].get_feature_names_out()\n",
    "features\n",
    "\n",
    "new_features = [i.replace(\"onehotencoder__\",\"\").replace(\"remainder__\", \"\") for i in features]\n",
    "new_features\n",
    "\n",
    "\n",
    "model = GradientBoostingClassifier(learning_rate=0.5, max_features=5, n_estimators=300,\n",
    "                           random_state=42)\n",
    "operations = [(\"transformer\", transformer), (\"gb\", model)]\n",
    "pipe_model = Pipeline(steps=operations)\n",
    "# Fit the pipeline\n",
    "pipe_model.fit(X_train, y_train)\n",
    "\n",
    "# Access feature importances from the GradientBoostingClassifier\n",
    "gb_model = pipe_model.named_steps['gb']  # Access the GradientBoostingClassifier step\n",
    "feature_importances = gb_model.feature_importances_\n",
    "\n",
    "# Get feature names from the transformer\n",
    "feature_names = pipe_model.named_steps['transformer'].get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feats = pd.DataFrame(index=feature_names, data=feature_importances, columns=['gb_feature_importance'])\n",
    "\n",
    "# Sort the features by importance\n",
    "grad_imp_feats = feats.sort_values(\"gb_feature_importance\", ascending=False)\n",
    "\n",
    "# Display the sorted feature importances\n",
    "print(grad_imp_feats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
